{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-30-2020\n",
    "\n",
    "\n",
    "# TWEETINSIGHTS - PART 1 - Collecting tweets\n",
    "\n",
    "### ALEX MAZZARELLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SCIENCE full time course - BrainStation\n",
    "### CAPSTONE PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the code in this notebook, is to collect tweets in a csv file, through the Twitter API.\n",
    "\n",
    "We will create \n",
    "\n",
    "* a function to collect tweets\n",
    "* additional code that calls the collecting function repeated times.\n",
    "\n",
    "We will use the **python-twitter API**.\n",
    "\n",
    "A few points to keep in consideration:\n",
    "\n",
    "* Twitter API has limits to both the number of requests sent (currently is 180 every 15 minutes) as well as to the total amount of tweets that we can download (currently is 500,000 every 30 days)\n",
    "\n",
    "* With the request we can specify the search terms. The API will return a JSON with the full metadata per each tweet. It is up to us to then eventually keep only those we are interested in, for the purpose of our project.\n",
    "\n",
    "To check the most updated API rules and options click [here](https://developer.twitter.com/en/products/twitter-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import twitter\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining function to collect tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(tw_api, search_term):\n",
    "    '''\n",
    "    Returns a pandas dataframe with tweet data obtained from request.\n",
    "    \n",
    "    Input:\n",
    "    - twitter API instance\n",
    "    - search term (string). Note, the current version of the function does not verify \n",
    "    that the search term passed is a string. Assertion will be implemented at next function update\n",
    "    \n",
    "    Output:\n",
    "    - Up to 50 tweets' metadata information in a pandas dataframe format\n",
    "    \n",
    "    Function sends a request to Twitter API, with the specified search string, for the most recent\n",
    "    50 tweets (in English language).\n",
    " \n",
    "    '''\n",
    "       \n",
    "    # initialize DataFrame to store tweet metadata returned\n",
    "    df = pd.DataFrame() \n",
    "\n",
    "    # setting search parameters for API request    \n",
    "    search_res = tw_api.GetSearch(\n",
    "                        #search term (passed as function parameter)\n",
    "                        term = search_term,\n",
    "                        # selecting language of tweets (en = english)\n",
    "                        lang = 'en',\n",
    "                        # most recent N tweets\n",
    "                        result_type = 'recent',\n",
    "                        # entities might include additional tweet info,\n",
    "                        # like hashtags, urls, user mentions, symbols\n",
    "                        include_entities = True,\n",
    "                        # specifying format of data returned\n",
    "                        return_json = True,\n",
    "                        # max number of tweets to return\n",
    "                        count = 50 \n",
    "                    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # flagging retweets and getting their full original text\n",
    "    for tw in search_res['statuses']:\n",
    "        \n",
    "        # for each tweet returned, check if it was a truncated retweet.\n",
    "        # need to use a try/except to avoid the execution to stop.\n",
    "        try:\n",
    "            # storing in a local var the original full text content\n",
    "            tw_txt = tw['retweeted_status']['full_text']\n",
    "            retweet = 1\n",
    "        except:\n",
    "            # no retweet\n",
    "            tw_txt = tw['full_text']\n",
    "            retweet = 0\n",
    "\n",
    "        # store selected tweet metadata content to df\n",
    "        df = df.append(\n",
    "            {\n",
    "            'tweet_id': tw['id'],\n",
    "            'user_location': tw['user']['location'],\n",
    "            'created_at': tw['created_at'],\n",
    "            'handle': tw['user']['screen_name'],\n",
    "            'tweet': tw_txt,\n",
    "            'retweet': retweet,\n",
    "            'hashtags': tw['entities']['hashtags'],\n",
    "            'symbols': tw['entities']['symbols'],\n",
    "            'user_mentions': tw['entities']['user_mentions'],\n",
    "            'followers_count': tw['user']['followers_count'],\n",
    "            'friends_count': tw['user']['friends_count']\n",
    "            \n",
    "            },\n",
    "            ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some manual tuning I found out that a good request rate to balance the number of unique and duplicated results, is \n",
    "\n",
    "* 50 tweets per request\n",
    "* one request every 3 minutes\n",
    "* using two search terms alternatively\n",
    "\n",
    "Note, that is for two brands (Netflix, Twitter).\n",
    "\n",
    "Since the number of tweets containing different search terms may vary, it is recommendable to run a couple tests to fine tune the parameters above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multiple requests cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will set up the instructions for the code to:\n",
    "\n",
    "* initialize an API connection\n",
    "* define API search terms\n",
    "* define requests frequency intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the code below is optimized for running requests for two brands (in our case Netflix and Xbox), therefore returning one separate CSV for each brand, with tweets info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request n 1 completed - tweets added to net_tweets_pulled/net_Tweets20201029-162357.csv and xbox_tweets_pulled/xbox_Tweets20201029-162358.csv -- Search term used: @netflix -from:netflix and @xbox -from:xbox\n",
      "countdown until next request...\n",
      "Request n 2 completed - tweets added to xbox_tweets_pulled/xbox_Tweets20201029-162358.csv -- Search term used: #xbox -from:xbox\n",
      "countdown until next request...\n",
      "167  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8e6ec88c7383>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{t}  \\r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' \\r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########### Local variables declaration ##########\n",
    "\n",
    "\n",
    "# total number of requests to send - must be int\n",
    "num_requests = 240 \n",
    "# the wait time in between requests (expressed in seconds, must be int)\n",
    "interval = 180 \n",
    "\n",
    "\n",
    "# search terms list Netflix - must be at least two!\n",
    "src_terms_net = ['@netflix -from:netflix','#netflix -from:netflix'] \n",
    "# search terms counter - Netflix\n",
    "src_term_cnt_net = 0\n",
    "# variable used for storing csv file name (Netflix)\n",
    "csv_net_name = '' \n",
    "\n",
    "\n",
    "# search terms list xbox - must be at least two!\n",
    "src_terms_xbox= ['@xbox -from:xbox','#xbox -from:xbox'] \n",
    "# search terms counter - xbox\n",
    "src_term_cnt_xbox= 0\n",
    "# variable used for storing csv file name (xbox)\n",
    "csv_xbox_name = '' \n",
    "\n",
    "\n",
    "# string used to print, for each request completed at which csv file the tweets have been apended \n",
    "tw_added_to_csv = '' \n",
    "# string used to print, for each request completed what was the search term used \n",
    "tw_src_term_used = '' \n",
    "\n",
    "\n",
    "##########  API authentication  ##########\n",
    "\n",
    "# initializing parameters for authentication to Twitter API - insert your own credentials\n",
    "tw_api = twitter.Api(\n",
    "                        consumer_key = '',\n",
    "                        consumer_secret = '',\n",
    "                        access_token_key = '',\n",
    "                        access_token_secret = '',\n",
    "                        # returns full (non truncated) tweets\n",
    "                        tweet_mode = 'extended'  \n",
    "                        ) \n",
    "\n",
    "# check my API credentials\n",
    "try:\n",
    "    tw_api.VerifyCredentials()\n",
    "except:\n",
    "    print('API authentication error')\n",
    "\n",
    "\n",
    "#################### Sending API requests ####################\n",
    "    \n",
    "for i in range(1, num_requests+1):\n",
    "    \n",
    "    ########## First cycle of loop ##########\n",
    "    \n",
    "    # send a request for each of the two brands and create respective CSVs\n",
    "    \n",
    "    if i==1:\n",
    "        # sends request to API with Netflix search terms\n",
    "        df_req_net = scrape_tweets(tw_api, src_terms_net[src_term_cnt_net])\n",
    "        # create csv files for the Netflix tweets  and writes tweets returned by request\n",
    "        csv_net_name += 'net_tweets_pulled/net_Tweets' + datetime.now().strftime('%Y%m%d-%H%M%S') +'.csv'\n",
    "        df_req_net.to_csv(csv_net_name, index = False)\n",
    "                \n",
    "        # same for xbox:\n",
    "        # sends request to API with Xbox search terms\n",
    "        df_req_xbox= scrape_tweets(tw_api, src_terms_xbox[src_term_cnt_xbox])\n",
    "        # create CSV for xbox tweets and writes tweets returned by request\n",
    "        csv_xbox_name += 'xbox_tweets_pulled/xbox_Tweets' + datetime.now().strftime('%Y%m%d-%H%M%S') +'.csv'\n",
    "        df_req_xbox.to_csv(csv_xbox_name, index = False)\n",
    "        \n",
    "        \n",
    "        # sets names of CSVs updated in the string variable that will be used for print statement\n",
    "        tw_added_to_csv = csv_net_name + ' and ' + csv_xbox_name\n",
    "        # sets search terms used in the string variable that will be used for print statement\n",
    "        tw_src_term_used = src_terms_net[src_term_cnt_net] + ' and ' + src_terms_xbox[src_term_cnt_xbox]\n",
    "        \n",
    "        # increases counter for Netflix search terms - list must be greater than one!\n",
    "        src_term_cnt_net += 1\n",
    "        # increases counter for xbox search terms - list must be greater than one!\n",
    "        src_term_cnt_xbox+= 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########## from the second cycle of the loop ##########\n",
    "    \n",
    "    # in the following block, we are\n",
    "    # calling the scraping function, getting a pandas dataframe with tweets returned\n",
    "    # it will alternate call functions for Netflix (odd loop cycles) and xbox (even loop cycles)\n",
    "    \n",
    "    # Odd loop cycles except the first one: calls scrape function for Netflix \n",
    "    if i%2 != 0 and i != 1:\n",
    "        df_req_net = scrape_tweets(tw_api, src_terms_net[src_term_cnt_net])\n",
    "        # appends df returned from scrape function to the csv created in the current loop cycle\n",
    "        df_req_net.to_csv(csv_net_name, mode = 'a', header = False, index = False)\n",
    "        \n",
    "        # sets names of CSV updated in the string variable that will be used for print statement\n",
    "        tw_added_to_csv = csv_net_name\n",
    "        # sets search term used in the string variable that will be used for print statement\n",
    "        tw_src_term_used = src_terms_net[src_term_cnt_net]\n",
    "        \n",
    "        # if the counter of search terms for Netflix points at the last term in the list: \n",
    "        # else increase by one\n",
    "        if src_term_cnt_net == len(src_terms_net)-1:\n",
    "            src_term_cnt_net = 0\n",
    "        else: src_term_cnt_net += 1\n",
    "    \n",
    "    # even loop cycles (and excludes first cycle): calls scrape function for xbox\n",
    "    elif i != 1:\n",
    "        df_req_xbox= scrape_tweets(tw_api, src_terms_xbox[src_term_cnt_xbox])\n",
    "        # appends df returned from scrape function to the csv created in the current loop cycle\n",
    "        df_req_xbox.to_csv(csv_xbox_name, mode = 'a', header = False, index = False)\n",
    "        \n",
    "        # sets names of CSVs updated in the string variable that will be used for print statement\n",
    "        tw_added_to_csv = csv_xbox_name\n",
    "        # sets search term used in the string variable that will be used for print statement\n",
    "        tw_src_term_used = src_terms_xbox[src_term_cnt_xbox]\n",
    "        \n",
    "        # if the counter of search terms for xbox points at the last term in the list: reset,\n",
    "        # else increase by one\n",
    "        if src_term_cnt_xbox== len(src_terms_xbox)-1:\n",
    "            src_term_cnt_xbox = 0\n",
    "        else: src_term_cnt_xbox+= 1\n",
    "    \n",
    "    \n",
    "        \n",
    "    # confirm request and csv create/append with print statement\n",
    "    print(f'Request n {i} completed - tweets added to {tw_added_to_csv} -- Search term used: {tw_src_term_used}')\n",
    "    \n",
    "    # setting a delay timer to pause until the next request\n",
    "    # and printing countdown \n",
    "    print('countdown until next request...')\n",
    "    for t in reversed(range(1,interval+1)):\n",
    "        print(f'{t}  \\r', end=\"\")\n",
    "        time.sleep(1)\n",
    "    print(' \\r', end='')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above was run just to show an example of the print statements, which inform the user about the progress of data collection (search key used and csv files updated). \n",
    "\n",
    "**NOTE:** you will NOT find in any of the delivered folders the csv files listed in those print statements.\n",
    "Since they have been created just before submission, I have removed them right after running the example, so that they would not interfere with the rest of the analysis performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

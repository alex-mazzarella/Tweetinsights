{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-30-2020\n",
    "\n",
    "\n",
    "# TWEETINSIGHTS - PART 1bis\n",
    "\n",
    "### ALEX MAZZARELLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SCIENCE full time course - BrainStation\n",
    "### CAPSTONE PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for the reader: this is a version of the tweet collection code adapted to return all tweets in one csv only.\n",
    "\n",
    "As initially I had planned to run the analysis for two brands, I had created the code that would return two separate CSVs for two separate brands.\n",
    "\n",
    "But since in the end i run the sentiment and topic analysis for only one, I have added this current version of scraping, to be used if the user needs all the tweets returned in one single file.\n",
    "\n",
    "(I have not used this notebook for data collection in the capstone project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the code in this notebook, is to collect tweets in a csv file, through the Twitter API.\n",
    "\n",
    "We will create \n",
    "\n",
    "* a function to collect tweets\n",
    "* additional code that calls the collecting function repeated times.\n",
    "\n",
    "We will use the **python-twitter API**.\n",
    "\n",
    "A few points to keep in consideration:\n",
    "\n",
    "* Twitter API has limits to both the number of requests sent (currently is 180 every 15 minutes) as well as to the total amount of tweets that we can download (currently is 500,000 every 30 days)\n",
    "\n",
    "* With the request we can specify the search terms. The API will return a JSON with the full metadata per each tweet. It is up to us to then eventually keep only those we are interested in, for the purpose of our project.\n",
    "\n",
    "To check the most updated API rules and options click [here](https://developer.twitter.com/en/products/twitter-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import twitter\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining function to collect tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(tw_api, search_term):\n",
    "    '''\n",
    "    Returns a pandas dataframe with tweet data obtained from request.\n",
    "    \n",
    "    Input:\n",
    "    - twitter API instance\n",
    "    - search term (string). Note, the current version of the function does not verify \n",
    "    that the search term passed is a string. Assertion will be implemented at next function update\n",
    "    \n",
    "    Output:\n",
    "    - Up to 50 tweets' metadata information in a pandas dataframe format\n",
    "    \n",
    "    Function sends a request to Twitter API, with the specified search string, for the most recent\n",
    "    50 tweets (in English language).\n",
    " \n",
    "    '''\n",
    "       \n",
    "    # initialize DataFrame to store tweet metadata returned\n",
    "    df = pd.DataFrame() \n",
    "\n",
    "    # setting search parameters for API request    \n",
    "    search_res = tw_api.GetSearch(\n",
    "                        #search term (passed as function parameter)\n",
    "                        term = search_term,\n",
    "                        # selecting language of tweets (en = english)\n",
    "                        lang = 'en',\n",
    "                        # most recent N tweets\n",
    "                        result_type = 'recent',\n",
    "                        # entities might include additional tweet info,\n",
    "                        # like hashtags, urls, user mentions, symbols\n",
    "                        include_entities = True,\n",
    "                        # specifying format of data returned\n",
    "                        return_json = True,\n",
    "                        # max number of tweets to return\n",
    "                        count = 50 \n",
    "                    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # flagging retweets and getting their full original text\n",
    "    for tw in search_res['statuses']:\n",
    "        \n",
    "        # for each tweet returned, check if it was a truncated retweet.\n",
    "        # need to use a try/except to avoid the execution to stop.\n",
    "        try:\n",
    "            # storing in a local var the original full text content\n",
    "            tw_txt = tw['retweeted_status']['full_text']\n",
    "            retweet = 1\n",
    "        except:\n",
    "            # no retweet\n",
    "            tw_txt = tw['full_text']\n",
    "            retweet = 0\n",
    "\n",
    "        # store selected tweet metadata content to df\n",
    "        df = df.append(\n",
    "            {\n",
    "            'tweet_id': tw['id'],\n",
    "            'user_location': tw['user']['location'],\n",
    "            'created_at': tw['created_at'],\n",
    "            'handle': tw['user']['screen_name'],\n",
    "            'tweet': tw_txt,\n",
    "            'retweet': retweet,\n",
    "            'hashtags': tw['entities']['hashtags'],\n",
    "            'symbols': tw['entities']['symbols'],\n",
    "            'user_mentions': tw['entities']['user_mentions'],\n",
    "            'followers_count': tw['user']['followers_count'],\n",
    "            'friends_count': tw['user']['friends_count']\n",
    "            \n",
    "            },\n",
    "            ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some manual tuning I found out that a good request rate to balance the number of unique and duplicated results, is \n",
    "\n",
    "* 50 tweets per request\n",
    "* one request every 3 minutes\n",
    "* using two search terms alternatively\n",
    "\n",
    "Since the number of tweets containing different search terms may vary, it is recommendable to run a couple tests to fine tune the parameters above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multiple requests cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will set up the instructions for the code to:\n",
    "\n",
    "* initialize an API connection\n",
    "* define API search terms\n",
    "* define requests frequency intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request n 1 completed - tweets added to brand-name_tweets_pulled/brand-name_Tweets20201106-151919.csv -- Search term used: my_first_search_term\n",
      "countdown until next request...\n",
      "Request n 2 completed - tweets added to brand-name_tweets_pulled/brand-name_Tweets20201106-151919.csv -- Search term used: my_second_search_term\n",
      "countdown until next request...\n",
      "8   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fd5a0c5d8860>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{t}  \\r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' \\r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########### Local variables declaration ##########\n",
    "\n",
    "\n",
    "# total number of requests to send - must be int\n",
    "num_requests = 240 \n",
    "# the wait time in between requests (expressed in seconds, must be int)\n",
    "interval = 10 \n",
    "\n",
    "\n",
    "# search terms list - must be at least one\n",
    "src_terms = ['my_first_search_term', 'my_second_search_term', 'another_search_term'] \n",
    "# search terms counter\n",
    "src_term_cnt = 0\n",
    "# variable used for storing csv file name\n",
    "csv_name = '' \n",
    "\n",
    "\n",
    "# string used to print, for each request completed at which csv file the tweets have been apended \n",
    "tw_added_to_csv = '' \n",
    "# string used to print, for each request completed what was the search term used \n",
    "tw_src_term_used = '' \n",
    "\n",
    "\n",
    "##########  API authentication  ##########\n",
    "\n",
    "# initializing parameters for authentication to Twitter API - insert your own credentials\n",
    "tw_api = twitter.Api(\n",
    "                        consumer_key = '',\n",
    "                        consumer_secret = '',\n",
    "                        access_token_key = '',\n",
    "                        access_token_secret = '',\n",
    "                        # returns full (non truncated) tweets\n",
    "                        tweet_mode = 'extended'  \n",
    "                        ) \n",
    "\n",
    "# check my API credentials\n",
    "try:\n",
    "    tw_api.VerifyCredentials()\n",
    "except:\n",
    "    print('API authentication error')\n",
    "\n",
    "\n",
    "#################### Sending API requests ####################\n",
    "    \n",
    "for i in range(1, num_requests+1):\n",
    "    \n",
    "    ########## First cycle of loop ##########\n",
    "    \n",
    "    # send a first request and create CSV\n",
    "    \n",
    "    if i==1:\n",
    "        # sends request to API with  search terms\n",
    "        df_req = scrape_tweets(tw_api, src_terms[src_term_cnt])\n",
    "        # create csv files for the pulled tweets tweets  and writes tweets returned by request\n",
    "        csv_name += 'brand-name_tweets_pulled/brand-name_Tweets' + datetime.now().strftime('%Y%m%d-%H%M%S') +'.csv'\n",
    "        df_req.to_csv(csv_name, index = False)\n",
    "                \n",
    "     \n",
    "        \n",
    "        # sets names of CSV updated in the string variable that will be used for print statement\n",
    "        tw_added_to_csv = csv_name \n",
    "        # sets search terms used in the string variable that will be used for print statement\n",
    "        tw_src_term_used = src_terms[src_term_cnt]\n",
    "        \n",
    "        # if the counter of search terms points at the last term in the list: \n",
    "        # else increase by one\n",
    "        if src_term_cnt == len(src_terms)-1:\n",
    "            src_term_cnt = 0\n",
    "        else: src_term_cnt += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########## from the second cycle of the loop ##########\n",
    "\n",
    "    # in the following block, we are\n",
    "    # calling the scraping function, getting a pandas dataframe with tweets returned\n",
    "    \n",
    "    else:\n",
    "        df_req = scrape_tweets(tw_api, src_terms[src_term_cnt])\n",
    "        # appends df returned from scrape function to the csv created in the current loop cycle\n",
    "        df_req.to_csv(csv_name, mode = 'a', header = False, index = False)\n",
    "        \n",
    "        # sets names of CSV updated in the string variable that will be used for print statement\n",
    "        tw_added_to_csv = csv_name\n",
    "        # sets search term used in the string variable that will be used for print statement\n",
    "        tw_src_term_used = src_terms[src_term_cnt]\n",
    "        \n",
    "        # if the counter of search terms points at the last term in the list: \n",
    "        # else increase by one\n",
    "        if src_term_cnt == len(src_terms)-1:\n",
    "            src_term_cnt = 0\n",
    "        else: src_term_cnt += 1\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "        \n",
    "    # confirm request and csv create/append with print statement\n",
    "    print(f'Request n {i} completed - tweets added to {tw_added_to_csv} -- Search term used: {tw_src_term_used}')\n",
    "    \n",
    "    # setting a delay timer to pause until the next request\n",
    "    # and printing countdown \n",
    "    print('countdown until next request...')\n",
    "    for t in reversed(range(1,interval+1)):\n",
    "        print(f'{t}  \\r', end=\"\")\n",
    "        time.sleep(1)\n",
    "    print(' \\r', end='')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above was run just to show an example of the print statements, which inform the user about the progress of data collection (search key used and csv files updated). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
